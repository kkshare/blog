---
layout: post
title: "plog"
description: "log parser,python编写用于日志处理"
category: "tech"
tags: [python,日志服务器,plog]
tagline: "2016-04-15"

---
{% include JB/setup %}

https://github.com/SinaMSRE/Plog.git

代码见 [这里](/files/pyplog.tgz)

## 设计思路

### 管道(pipe)

- 管道通信遵循先进先出的原理，并且数据只能被读取一次，当此段数据被读取后，马上会从数据中消失
- 管道是一种半双工的通信机制，也就是说，它只能一端用来读，另外一端用来写。（建立两条管道实现全双工）
- 匿名管道只能用来在具有公共祖先的两个进程之间通信
- 命名管道(fifo)可以让任意的进程之间实现通信

### 实时处理滚动日志文件

文件重命名或移动之后inode不变，但已经打开的文件仍然从原文件读取数据。所以

- 日志文件以大小分割，200M-500M一个文件
- 系统处理未分割的文件，即最新文件
- 如果没有读取到数据，那么检查inode是否变化，如果inode变化说明文件发生分割
- 此时按文件名重新打开文件即可
- 以上保证文件处理尽量实时，不需要备份已经处理的文件（由日志生产者处理）

### 避免重复入库

- 生产日志时为每条消息生成唯一ID，一边人工或机器查询
- 插入数据库时忽略重复数据(insert ignore into ...)
- 通过文件记录断点

### 断点处理

- 使用文件记录断点，效率较高
- 使用两个文件记录断点，交叉写，因为记录断点文件时是采用覆盖写的方式，避免数据被删但尚未写入时程序崩溃
- 记录处理日志文件的inode，便于区分此断点文件是否属于当前文件
- 程序启动时从断点处开始处理，并将断点文件内容记录到日志，以便查询

### 特殊字符问题

- 如果采用csv模块读文件，但是无法处理半行问题
- 所以只是在channel解析中处理特殊字符，可以用正则表达式(re.split)或grok模块，因比较复杂，没有研究成功
- 所以用StringIO 模块把内容转换成内存文件再用csv模块读取，性能有一些损失

### 性能

- 多线程：读数据，处理(统计、过滤、分割)数据任务与入库任务分开线程处理
- 读线程以及写线程通过队列共享数据，需要控制队列大小
- Python 的 queue 实现就是线程安全的，没有必要上锁

