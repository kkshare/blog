---
layout: post
title: "hadoop"
description: "hadoop"
category: "tech"
tags: [hadoop,nosql]

---
{% include JB/setup %}

版本：0.20.2  
依赖：JDK1.6 + (from oracle http://www.java.com/download/)  
网站：http://sourceforge.net/projects/hadooppp/  

## 部署  

1.域名配置

所有机器都通过域名（机器名）访问，保证域名与主机名一致

    vi /etc/sysconfig/network
    hostname xxx;
    vi /etc/hosts

2.每台机器之间都需要SSH无密码登录，包括本机登录到本机

3.配置

    vi slaves #添加slaves的主机名，回车分隔
    vi masters #添加masters的主机名，回车分隔
    vi core-site.xml #配置fs.default.name
    vi mapred-site.xml #配置mapred.job.tracker
    
vi hdfs-site.xml

    dfs.replication          1
    dfs.name.dir          /opt/hadoop/dfs/name
        If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy.
    dfs.data.dir          /opt/hadoop/dfs/data,/opt/hadoop/dfs/data1
        If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices.
    fs.checkpoint.dir          ${hadoop.install.dir}/dfs/secondaryname
        save checkpoint of secondary namenode.
    dfs.http.address          0.0.0.0:50070
        namenode http monitor.You can execute checkpoint anywhere if host is a domain, or only on masters.
    topology.script.file.name          /path/to/script
        Rack Awareness
    dfs.datanode.du.reserved          1073741824
        Reserved space in bytes per volume. Always leave this much space free for non dfs use.

vi hadoop-env.sh

    export JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk
    export HADOOP_PID_DIR=/var/hadoop/pids

4.文件分发：

     scp hadoop-0.20.2.tar.gz host*:/opt/
     scp –r conf host*:/opt/hadoop-conf
     for i in `cat /tmp/mynodes`;do scp * $i:/opt/hadoop;done
     for i in host1 host2 host3;do scp * $i:/opt/hadoop;done

5.安装：

     tar –zxvf hadoop-0.20.2.tar.gz
     ln -s /opt/hadoop-0.20.2 /opt/hadoop
     mkdir -p /data/proclog/dfs/data
     mkdir -p /data/data1/dfs/data
     mkdir -p /opt/hadoop/dfs
     mkdir -p /data/proclog/hadoop_logs
     mkdir -p /data/proclog/dfs/name
     ln -s /data/proclog/dfs/data /opt/hadoop/dfs/data
     ln -s /data/data1/dfs/data /opt/hadoop/dfs/data1
     ln -s /data/proclog/hadoop_logs /opt/hadoop/logs
     ln -s /data/proclog/dfs/name /opt/hadoop/dfs/name
     rm –rf /opt/hadoop/conf
     mv –f /opt/hadoop-conf /opt/hadoop/conf

6.文件系统格式化

     第一次启动系统前需要格式化文件系统
     /opt/hadoop/bin/hadoop namenode –format
     如果有旧版本数据需要清除，每台都要，否则datanode启动不起来(见${dfs.data.dir} ${fs.checkpoint.dir})

7.测试:

    系统启动：
        /opt/hadoop/bin/start-all.sh
    系统停止：
        /opt/hadoop/bin/stop-all.sh
    监控：
        http://localhost:50070/ #文件系统监控
        http://localhost:50030/ #任务监控

测试：

     cd /opt/hadoop
     bin/hadoop dfs –put conf input
     bin/hadoop jar hadoop-0.20.2-examples.jar wordcount input output
     bin/hadoop jar hadoop-0.20.2-examples.jar grep input output 'dfs[a-z.]+'#常见问题#

通过监控页面检查datanode是否启动，如果没有启动

- 检查防火墙是否关闭？service iptables stop
- 把safemode置于off状态：/opt/hadoop/bin/hadoop dfsadmin -safemode leave
- 也有可能是刚启动，datanode与namenode正在进行状态检测，所以可以等待几分钟。
- /etc/hosts 配置成0.0.0.0时不能启动namenode

### 加入HBASE

    vi hadoop-env.sh
    export HBASE_HOME=/opt/hbase
    export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HBASE_HOME/hbase-0.20.6.jar:$HBASE_HOME/hbase-0.20.6-test.jar:$HBASE_HOME/conf:${HBASE_HOME}/lib/zookeeper-3.2.2.jar
    scp to all slaves

隐藏文件: 以".","_"带头的文件/目录默认会被过滤掉

包名问题: core包必须命名成hadoop-*-core.jar

**Hadoop数据块Miss在页面提示的问题**

    WARNING : There are about 900 missing blocks. Please check the log or run fsck.

運行”hadoop fsck /“提示很多block corrupt/missing  
如果那些missing的block已經沒有用處，不需要恢復：

    hadoop fsck -move 會把有錯誤的檔案移到 HDFS 的 /lost+found
    hadoop fsck -delete 可以移除 missing block

如果想要恢復missing的block：就要看那幾個 block 是否還有存在某幾台 DataNode 的 ${dfs.data.dir} 目錄中了

拷贝份数手動設定為 3:
    
    hadoop fs -setrep -R -w 3 /

secondary namenode

    cp conf/masters conf/masters-backup
    vi bin/start-dfs.sh bin/stop-dfs.sh
    --hosts masters ==> --hosts masters-backup

**备份与恢复**

checkpoint[备份,可以解决重启NameNode时间过长的弊端]

    bin/hadoop secondarynamenode -checkpoint [force]

${fs.checkpoint.dir}生成备份文件

hdfs-site.xml配置dfs.http.address(默认0.0.0.0:50070)可在任何机器执行checkpoint,否则只能在namenode执行

Import Checkpoint(恢复数据):

    mkdir -p ${dfs.name.dir} //必须保证为空，否则出错
    scp -r secondarynamenode:${fs.checkpoint.dir} ${fs.checkpoint.dir}
    bin/hadoop namenode -importCheckpoint

**LZO-install**

参考hbase部署

**安全删除节点**

1. 在Master vi conf/excluded-list
2. vi conf/hdfs-site.xml //set dfs.hosts.exclude=conf/excluded-list
3. bin/hadoop dfsadmin -refreshNodes //监控页面查看状态，等待完成

**机架感知配置**

    topology.script.file.name

**支持最大文件数**

由namenode内存决定，Every file, directory and block in HDFS is represented as an object.  
Each file object and each block object takes about 150 bytes of the memory.  
假设10亿个文件，每个文件占一个block,namenode需要3G内存  
存取小文件需要比较多的seek操作，而且经常从datanode指间跳动，效率较低  
可解决很多小文件问题：[http://www.cloudera.com/blog/2009/02/the-small-files-problem/](http://www.cloudera.com/blog/2009/02/the-small-files-problem/)  
    Hadoop Archives (HAR files)、HBase、Sequence Files

### 基础理论

磁盘数据块的大小是磁盘读写的最小单位，通常是512字节；  
文件系统数据块是文件系统处理数据的单位，块通常是几KB(df/fsck可维护文件系统，对文件块进行操作)  
HDFS的文件块默认大小达64MB，与常规文件系统不同，小于文件块大小的HDFS文件不会占据整个文件块的空间。这是为了最小化磁盘寻址开销，另外也不能过大，因为Map任务通常一次处理一个文件块的数据，数据块过大会降低并发量。好处见《云计算与数据中心建设 - Hadoop概述.pdf》

**一致性模型**  
在创建一个文件之后，在文件系统的命名空间中是可见的  
但写入文件的内容并不保证能被看见，即使数据流已经被刷新  
当写入的数据超过一个块的数据，其他读取者才能看见第一个块。对于之后的块也是如此。总之，其他读取者始终是看不见当前正在被写入的块  

#### hadoop mapreduce执行流程

以wordcount为例，步骤如下：

1. 每个map任务使用默认的TextInputformat类的LineRecordReader方法按行读取文件，这个读取的行数据就被交给map函数去执行
2. 如果有combine，先对第一步的输出结果就行combine操作。Combine就是个小reduce操作
3. 每个map对自己的输出文件进行patition操作。生成10个文件，假设是r1、r2….r10这10个文件
4. copy文件：reducer1会从100台map机器上取到所有r1文件，reducer2取所有r2的文件...
5. 每个reducer合并（meger）自己取到的文件，reducer1就是合并100个r1文件（实际过程是在上面第4步操作中会边copy边meger，在内存中）。
6. 合并好后进行下sort（排序）操作，再次把不同小文件中的同一个单词聚合在一起。作为提供给reduce操作的数据。
7. 进行reduce操作，对同一个单词的value列表再次进行累加，最终得到某个单词的词频数。
8. Outputformat操作，把reduce结果写到磁盘。

所以，总的流程应该是这样的：

    map阶段reduce阶段
    Inputformat—>map—>(combine)—>partition—>
    copy&merge—>sort—>reduce—>outputformat
    (shuffle:copy&merge)

由此我们也可以看出，执行reduce的代价还是有些的，所以如果我们的应用只使用map就能搞定的话，那就尽量不要再有reduce操作在其中。

