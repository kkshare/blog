---
layout: post
title: "hbase"
description: "hbase"
category: "tech"
tags: [hbase,thrift,zookeeper]
tagline: "2016-04-15"

---
{% include JB/setup %}

版本：0.20.6  
**Requirements**:

1.ZooKeeper  
2.NTP  
3.patch  
4.ulimit -n //需要打开大量文件,最好大于10k  
5.版本需要与hadoop配套

## 部署步骤

1.配置

    vi hbase-env.sh
    export JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk
    export HBASE_PID_DIR=/var/hadoop/pids

    use the HBASE_MANAGES_ZK variable in ${HBASE_HOME}/conf/hbase-env.sh. This variable, which defaults to true, tells HBase whether to start/stop the ZooKeeper quorum servers alongside the rest of the servers.
    vi hadoop/conf/hdfs-site.xml #dfs.datanode.max.xcievers=2047,该参数限制了datanode所允许同时执行的发送和接受任务的数量，默认256    

vi hbase-site.xml

    hbase.rootdir          hdfs://CTC-BJ-6:9000/hbase
         The directory shared by region servers.
    hbase.cluster.distributed          true
         The mode the cluster will be in. Possible values are
         false: standalone and pseudo-distributed setups with managed Zookeeper
         true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)
    hbase.zookeeper.property.clientPort          2222
         Property from ZooKeeper's config zoo.cfg.The port at which the clients will connect.
    hbase.zookeeper.quorum          rs1.example.com,rs2.example.com,rs3.example.com
         Comma separated list of servers in the ZooKeeper Quorum.
         For example, "host1.mydomain.com,host2.mydomain.com,host3.mydomain.com".
         By default this is set to localhost for local and pseudo-distributed modes
         of operation. For a fully-distributed setup, this should be set to a full
         list of ZooKeeper quorum servers. If HBASE_MANAGES_ZK is set in hbase-env.sh
         this is the list of servers which we will start/stop ZooKeeper on.
    hbase.zookeeper.property.dataDir          /opt/hbase/zookeeper
         Property from ZooKeeper's config zoo.cfg.
         The directory where the snapshot is stored.
         By default, it will be stored in /tmp which is sometimes cleaned in live systems
    
    vi regionservers #将hadoop/conf/slaves加入其中

2.安装

    wget http://labs.renren.com/apache-mirror//hbase/hbase-0.20.6/hbase-0.20.6.tar.gz
    tar –zxvf hbase-0.20.6.tar.gz
    ln -s /opt/hbase-0.20.6 /opt/hbase
    mkdir -p /data/proclog/zookeeper
    ln -s /data/proclog/zookeeper /opt/hbase/zookeeper
    scp conf conf//zookeeper.quorum regionservers

3.启动

    在Hbase的master执行${HBASE_HOME}/bin/start-hbase.sh

4.测试

    ${HBASE_HOME}/bin/hbase shell
     
### faq

**Eclipse开发**  
将配置文件目录添加到classpath中(run->run configurations...->classpath->advance...->add external floder)

**Master**  
Hbase没有master仍然能够运行，但是不能分片. 如果master down了在任何一个正确配置的机器上执行

    ${HBASE_HOME}/bin/hbase-daemon.sh start master

新启动的机器能够执行master的功能.  
支持多个master的启动。

**与hdfs client一起工作**  
执行下列之一：  

- Add a pointer to your HADOOP_CONF_DIR to CLASSPATH in hbase-env.sh.    
- Add a copy of hdfs-site.xml (or hadoop-site.xml) to ${HBASE_HOME}/conf, or
- if only a small set of HDFS client configurations, add them to hbase-site.xml.

*启动停止*  
停止Hadoop之前必须先停止hbase.

*ZooKeeper*  
It is recommended to run a ZooKeeper quorum of 3, 5 or 7 machines, and give each ZooKeeper server around 1GB of RAM, and if possible, its own dedicated disk.
非托管：

- set HBASE_MANAGES_ZK in ${HBASE_HOME}/conf/hbase-env.sh to false
- add a suitably configured zoo.cfg to the CLASSPATH. HBase will see this file and use it to figure out where ZooKeeper is.
- 启动：${HBASE_HOME}/bin/hbase-daemons.sh {start,stop} zookeeper
- /etc/hosts配置：一个IP有多个域名不能配置多行，否则不能启动zookeeper
- Hbase可能不能共享zookeeper：导致生成错误的hdfs://ip:9000/hbase/hbase.version，需要删除后再启动hbase,不然不能启动hbase

**制作客户端**：（使支持命令 bin/hbase jar a.jar:b.jar CLASSNAME param）  

     vi bin/hbase 197行加入
     elif [ "$COMMAND" = "jar" ] ; then
     CLASSPATH=${CLASSPATH}:$1
     shift
     CLASS=$1
     shift

**thrift API生成**   
安装thritf（Linux）:http://incubator.apache.org/thrift/

    wget http://labs.renren.com/apache-mirror//incubator/thrift/0.5.0-incubating/thrift-0.5.0.tar.gz
    tar --gzip -xvf thrift-0.5.0.tar.gz
    cd thrift-0.5.0
    ./configure
    make
    make install
    cd lib/py
    setup.py install

生成hbase的client代码:

    cd $HBASE_HOME/src/java/org/apache/hadoop/hbase/thrift
    thrift --gen py Hbase.thrift

然后将生成的gen-py文件夹下的hbase文件夹拷贝到/usr/lib/python2.5/site-packages/

准备hbase:  
首先确认hbase正常工作，然后启动hbase的thrift服务：

    $HBASE_HOME/bin/hbase-deamon.sh start thrift

参考例子$HBASE_HOME/src/examples/thrift/DemoClient.py

### 软硬件环境

     # 存储主节点（NameNode）和备份主节点(SecondaryNameNode)
        1. 内存：>=24G
        2. CPU：不要太低，8核也可以
        3. 硬盘：需要做raid，空间要求不大，上百G的就可以
        4. 网络：千兆网卡即可，如果有条件，就直接上到更高的
     # 计算主节点（SecondaryNameNode）
        1. 内存：>=24G
        2. CPU：尽量高一些
        3. 硬盘：空间要求不大，上百G的就可以
        4. 网络：千兆网卡即可，如果有条件，就直接上到更高的
     # 存储节点(DataNode)
        1. 内存要求：无
        2. CPU要求：无
        3. 硬盘要求：无特殊要求，不要特别小就可以
        4. 网络：千兆网卡，需要配置成千兆如果宕成百兆则直接把网卡宕掉，否则会影响整个集群的性能
     #  计算节点(TaskTracker)
        1. 内存要求：>=16G，保证最低计算任务需求，16G根据云梯经验，配置6个map和6个reduce
        2. CPU要求：无
        3. 硬盘要求：无特殊要求，不要特别小就可以
        4. 网络：千兆网卡，需要配置成千兆如果宕成百兆则直接把网卡宕掉，否则会影响整个集群的性能

**高级配置**  
vi /etc/security/limits.conf文件，添加如下配置

    net.ipv4.tcp_max_syn_backlog = 16384
    net.ipv4.tcp_synack_retries = 5
    net.core.somaxconn = 32768
    net.core.rmem_default = 262144
    net.core.wmem_default = 262144
    net.core.netdev_max_backlog = 10000
    net.core.rmem_max = 16777216
    net.core.wmem_max = 16777216
    net.ipv4.tcp_rmem = 8192 87380 16777216
    net.ipv4.tcp_wmem = 8192 65536 16777216
    net.ipv4.tcp_mem = 8388608 12582912 16777216

#### 其他

    所有节点：ulimit –n 655360
    namenode：echo 1 >/proc/sys/vm/overcommit_memory

*HADOOP_HEAPSIZE*  
NameNode需要把HADOOP_HEAPSIZE修改为比较大，如24G内存的机器，可以选择>=20G的空间作为HADOOP_HEAPSIZE的值， 注意，该参数是以M为单位的；JobTracker则对内存要求不是很高，需要把 HADOOP_HEAPSIZE配置成>=5G即可；DataNode和TaskTracker可以先按照默认的配置1G启动，如果使用过程中发现问题再进行适当增大即可。

### LZO-install

1.gcc-4.1 is ok

2.ant-1.8.2 is ok

    cp -r apache-ant-1.8.2 /usr/local
    vi /etc/profile
    export ANT_HOME=/usr/local/apache-ant-1.8.2
    export PATH=$PATH:$ANT_HOME/bin
    source /etc/profile

3.lzo【所有机器安装】

    tar -zxvf lzo-2.04.tar.gz
    ./configure --enable-shared
    make && make install
    vi /etc/ld.so.conf.d/lzo.conf //写入lzo库文件的路径"/usr/local/lib"
    /sbin/ldconfig -v

4.lzo编码/解码器的安装【所有机器安装】

    wget http://packages.sw.be/lzo/lzo-devel-2.04-1.el5.rf.i386.rpm 
    wget http://packages.sw.be/lzo/lzo-2.04-1.el5.rf.i386.rpm 
    rpm -ivh lzo-2.04-1.el5.rf.i386.rpm
    rpm -ivh lzo-devel-2.04-1.el5.rf.i386.rpm

5.hadoop hbase接口

    wget http://hadoop-gpl-compression.apache-extras.org.codespot.com/files/hadoop-gpl-compression-0.1.0-rc0.tar.gz
    tar -zxvf hadoop-gpl-compression-0.1.0-rc0.tar.gz
    cd hadoop-gpl-compression-0.1.0
    cp /opt/hadoop/hadoop-*.jar lib
    ant compile-native tar     //64位机器编译之前需"export CFLAGS=-m64"
    scp build/hadoop-gpl-compression-0.1.0-dev.jar hadoop/lib //或 hadoop-gpl-compression-0.1.0.jar
    scp build/hadoop-gpl-compression-0.1.0-dev.jar hbase/lib
    scp -r lib/native/* hadoop/lib/native
    scp -r lib/native/* hbase/lib/native

vi core-site.xml

    io.compression.codecs =
        org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzopCodec
    io.compression.codec.lzo.class =
        com.hadoop.compression.lzo.LzoCodec

vi mapred-site.xml //mapreduce.map.output.compress 使mapreduce支持lzo 只有lzoTextInputFormat

**测试**  
执行以下命令检查，如果不支持的话这个命令会提示你还缺什么文件

    hbase org.apache.hadoop.hbase.util.CompressionTest hdfs://namenode:9000/test_path lzo

如果使用了CDH3版本的hdfs，那么该版本的hdfs与hadoop-gpl-compression会有冲突，
原因是CDH3修改了compression.java，增加了reinit()接口。
此时需要重新编译hadoop-gpl-compression工程，修改 src/java/com/hadoop/compression/lzo/LzoCompressor.java，增加以下行：

    public void reinit(Configuration conf) { 
    // do nothing 
    }

重新编译，拷贝

6.lzop工具-可以生产lzo文件-选装

    wget http://www.lzop.org/download/lzop-1.03.tar.gz 
    tar -zxvf lzop-1.03 
    cd lzop-1.03 
    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib 
    ./configure 
    make && make install 
             
### 测试优化

HBase的写效率还是很高的，但其随机读取效率并不高
可以采取一些优化措施来提高其性能，如：

1. 启用lzo压缩，见这里
2. 增大hbase.regionserver.handler.count数为100
3. 增大hfile.block.cache.size为0.4，提高cache大小
4. 增大hbase.hstore.blockingStoreFiles为15
5. 启用BloomFilter，在HBase0,89中可以设置
6. Put时可以设置setAutoFlush为false，到一定数目后再flushCommits

在14个Region Server的集群上，新建立一个lzo压缩表
测试的Put和Get的性能如下：

1. Put数据：  
单线程灌入1.4亿数据，共花费50分钟，每秒能达到4万个，这个性能确实很好了，不过插入的value比较小，只有不到几十个字节  
多线程put，没有测试，因为单线程的效率已经相当高了  

2. Get数据：  
在没有任何Block Cache，而且是Random Read的情况：  
单线程平均每秒只能到250个左右  
6个线程平均每秒能达到1100个左右  
16个线程平均每秒能达到2500个左右  

有BlockCache（曾经get过对应的row，而且还在cache中）的情况：
单线程平均每秒能到3600个左右  
6个线程平均每秒能达到1.2万个左右  
16个线程平均每秒能达到2.5万个左右  

**http://su.pr/网站的后台配置**

    ●两个集群
    ●实时数据服务：20个节点的集群服务每秒20,000个请求
    ●用MapReduce进行分析
    ●所有新的功能由HBase提供
    ●服务器硬件规格
         ●双核i7处理器
         ●24GB内存
         ●4 x 1 TB硬盘

●新Facebook Messages （“Project Titan”）使用HBase 0.90存储信息  
提到的好处：

    ●强一致性
    ●一个能够处理两种类型的数据模型的系统：
    –1) 一个不断增长但是很少访问的数据
    –2) 量相对较小但波动很大的数据
    ●大规模
    –每天新增200亿条消息
    –每月1350亿条邮件
    ●与Hadoop及Hive的整合
    ●端至端校验
    ●自动负载平衡和容灾
    ●支持数据压缩

**BUGS**:

删除数据后立即插入“相同时间撮”的数据可能导致失败，因为数据major compaction之前都被标记删除

**Tricks**：

    Put.setAutoFlush(false) htable.flushCommits() htable.close()
    Scan.setCaching(500)
    rs.close() //always close the ResultScanner!

研究HBASE的高手: http://www.larsgeorge.com/
    
一般情况故障率：CPU>MEN>DISK>MAINBORD,但ECC内存的错误率远低于磁盘坏道率，说明：如果磁盘不是RAID，则关闭swap分区。

