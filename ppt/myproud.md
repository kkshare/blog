title: 得意之作
speaker: kk
url: https://kkshare.github.io
transition: slide3
theme: dark

[slide]

# 得意之作
## 我做过的项目总结
<small>2016-06-15 何通庆</small>

[主页](https://kkshare.github.io)

[slide]
# OTS访问控制算法
<small>2015-10-16 智慧云</small>

[slide]
# 背景介绍
- 项目使用阿里云的NOSQL服务OTS
- 国庆前OTS数据迁移出现大量用户数据删除现象，因为
 * get_range时CU不足，导致数据读不出来
 * 判断数据是否读取完成的方式与旧版不同导致程序BUG
 
- 经过统计：  

日期|机器|单行操作出错数|批量操作出错数
----|----|----|----
24号|A   |50  |177218
24号|B   |2   |77093
25号|A   |395 |265865     
25号|B   |308 |218413

[slide]
# 可选OTS访问控制策略
- *事前控制*  
 需要独立服务器(中心)，系统复杂(服务器之间通讯、进程同步)
- *事后控制*  
 通过反馈，不需要中心，有一定延迟
- *综合控制*  
 事前计算大概参数，再根据反馈调整参数,算法有一定复杂度

[slide]
# 综合控制算法设计
- 5分钟内CU告警次数超过5次则降低putRowCountr/delRowCount
- 5分钟内无CU告警则增加putRowCount/delRowCount(优化IO)
- put与del分开是因为他们提交的数据大小不一样
- 提交之前等待（事前控制）
 ```java
     if(batchCount<10)
          Thread.sleep(500);
     else if(batchCount<20)
          Thread.sleep(400);
     else if(batchCount<30)
          Thread.sleep(300);
     else if(batchCount<50)
          Thread.sleep(200);
```
- 提交之后CU告警则等待并重试（事后控制）  
  等待间隔:350ms -> 700ms -> 1400ms,最多重试3次
- 每个表独立配置参数，因CU是对表的配置，不是对实例的配置

[slide]
# pyplog:日志解析服务
## 适合操作日志、历史日志入库与统计
<small>2015-10-16 智慧云</small>

[slide]
# plog 介绍
- https://github.com/SinaMSRE/Plog.git
- Python 开发处理日志流的框架
- 工程分成了三个部分:source,channel,sink
- source:处理数据来源，如读取file/socket等
- channel:数据预处理、分割、检查等，然后放入队列
- sink:从队列输出数据，比如入库

## plog 不能完全满足要求，仍然有很多问题需要解决

- 改造之后的代码 [这里](../../files/pyplog.tgz)

[slide]
# 项目背景
- OTS版本升级，导致历史数据保存在OTS不能满足要求
 * 建立索引不方便，满足不了查询(无字段名、字段值的过滤查询)
 * 单行纪录的字段个数有限制
－历史数据特点：
 * 属于冷数据，访问量少
 * 实时性要求比较低
 * 数据量大
 * 需要定期清理N天前的数据
- 决定将历史数据保存到mysql分区表

[slide]
# 方案1:使用管道(pipe)+入库程序
- 管道通信遵循先进先出的原理，并且数据只能被读取一次，当此段数据被读取后，马上会从数据中消失
- 管道是一种半双工的通信机制，也就是说，它只能一端用来读，另外一端用来写。（建立两条管道实现全双工）
- 匿名管道只能用来在具有公共祖先的两个进程之间通信
- 命名管道(fifo)可以让任意的进程之间实现通信
## 使用管道的缺点
 * 系统重启，或过载会丢失数据
 * 调试与定位问题不方便
 * 系统耦合紧密，通用性也不高

[slide]
# 方案2:使用日志文件+入库程序
## 需要解决的问题
- 日志是滚动的，文件会分割，文件名会变化
 * 文件重命名或移动之后inode不变
 * 文件重命名或移动之前打开的文件仍然从原文件读取数据
 * 日志文件以大小分割，200M-500M一个文件
 * 系统处理未分割的文件，即最新文件
 * 如果没有读取到数据，检查inode是否变化
 * inode变化说明文件发生分割，此时按文件名重新打开文件即可

[slide]
## 需要解决的问题
- 如何识别已经处理的文件
 * 通常的做法是将已经处理的文件移到别的目录
 * 以上机制保证文件处理非常实时，已经处理的文件可以不用管
- 如何避免重启等异常恢复之后的重复入库
 * 生产日志时为每条消息生成唯一ID，以便人工或机器查询
 * 插入数据库时忽略重复数据(insert ignore into ...)
 * 通过文件记录断点（称为断点文件）

[slide]
## 需要解决的问题
- 断点文件处理
 * 使用文件记录断点，效率较高
 * 使用两个文件记录断点，交叉写，因为记录断点文件时是采用覆盖写的方式，
 避免数据被删但尚未写入时程序崩溃
 * 记录处理日志文件的inode，  
 便于区分此断点文件是否属于当前文件
 * 程序启动时从断点处开始处理，  
 并将断点文件内容记录到日志，以便查询

[slide]
## 需要解决的问题
- 特殊字符与半行问题
 * 如果采用csv模块读文件，但是无法处理半行(数据不完整)问题
 * 所以只是在channel解析中处理特殊字符，可以用正则表达式(re.split)或grok模块，
 因比较复杂，没有研究成功
 * 用StringIO模块把内容转换成内存文件再用csv模块读取，  
 性能有一些损失

[slide]
## 需要解决的问题
- 性能
 * 多线程：读数据，处理(统计、过滤、分割)数据任务与入库任务分开线程处理
 * 读线程以及写线程通过队列共享数据，需要控制队列大小
 * Python 的 queue 实现就是线程安全的，没有必要上锁
 * 经过测试，平均速率达到3500条左右每秒
```
CPU:2.4GHz*16 MEM:8GB
dealed_total: 661317 usetime: 189.066871881 rate: 3497.79415833
dealed_total: 661517 usetime: 189.078732967 rate: 3498.63249884
dealed_total: 661717 usetime: 189.090950012 rate: 3499.46414652
```
- 改造之后的代码 [这里](../../files/pyplog.tgz)

[note]
3500条/s 意味着一天的历史几分钟可以处理完成
[/note]

[slide]
# JVM内存调优与GC优化

<small>2014-10-23 智慧云</small>

[slide]
# 项目背景
- 服务器抛出异常
```
java.lang.OutOfMemoryError: GC overhead limit exceeded`
```
- 检查服务器
```
# JVM运行参数
-Xmx512M -Xms512m -Xss1m -Xmx8000m -XX:PermSize=256M -XX:MaxPermSize=512M
-XX:ParallelGCThreads=20 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:SurvivorRatio=8
-XX:TargetSurvivorRatio=90 -XX:MaxTenuringThreshold=31
# GC日志
[GC 78113.538: [ParNew: 329967K->34374K(343936K), 0.0409480 secs] 2989192K->2693599K(8010088K), 0.0418980 secs]
```
 * JVM频繁触发垃圾回收
- 结论：我们的内存配置方案不合理

[slide]
# 原因分析
- 年轻代(343936K)到回收时已经快被占满，可能发生下面现象：
 * 新建对象在新生代放不下，于是放到年老代 
 * 从gc日志看到年老代不断增大，从几百兆增加到近8G，  
 然后产生CMS-GC回收，回收之后只剩几百兆
- 新生代(特别是EdenSpace)占用空间太小，导致
 * 频繁回收，GC占用过多CPU时间，影响正常业务
 * 使对象在S0与S1之间频繁转移，从而被升级到OldSpace，  
 实际上这些对象在新时代可以被回收
 * 在EdenSpace创建不了的对象被创建到了OldSpace，OldSpace长期占用内存

[slide]
# 解决方案
- 主要配置:
```
-Xms3g -Xmx6g -Xmn2g -Xss256k -XX:PermSize=64m -XX:MaxPermSize=128m -XX:ParallelGCThreads=12
```
 * `-Xmn2g`指定年轻代的大小(这个起到关键作用)
 * `-Xss256k`线程堆栈大小，线程数多时影响很大，需要测试
 * `-Xms3g`初始堆空间，避免每次垃圾回收后JVM重新分配内存
 * 永久区域内存64m-128m已经够用(依据tomcat日志)
 * 其它具体配置参考[博客java部分](../../tech/2016-04-12-java)
- 优化结果
```
# GC日志
2046.012: [GC 2046.012: [ParNew: 1761723K->184607K(1887488K), 0.0827150 secs] 2474376K->899094K(3042736K), 0.0843970 secs] [Times: user=0.29 sys=0.00, real=0.08 secs]
```
 * 新生代回收率能够达到超过80% ，已经比较理想。
 * 内存使用率从80%左右降到了20+％，不在频繁的触发GC

[slide]
# java应用CPU偶发性100%问题
```
top -H -p pid #查看哪些线程100%
jstack -F pid > cpu_100 #查看断点
```
[slide]
# 高清机顶盒下载状态算法
<small>2008 高清视频</small>

[slide]
# 项目背景
- 机顶盒需要显示一部电影的下载状态
- 一部电影通常有多个文件组成，包括多个视频与字幕
- 状态包括：等待、下载、暂停、完成、错误
- 难点：1.逻辑比较复杂 2.如何保证效率

[slide]
# 解决方法
- 制定根据自文件状态生成整部电影状态的规则  

子文件状态     | 整部电影状态
---------------|------------
都是等待       | 等待
等待 + 错误    | 等待
至少有一个下载 | 下载
都是暂停       | 暂停	
暂停 + 完成    | 暂停	
都是完成       | 完成
都是错误       | 错误
错误 + 完成    | 错误

[slide]
# 映射
- 制作子文件状态到整部电影状态的映射表(穷举法)
```bash
value| status  |            status of file
-----|---------|-------------------------------------------------------------
1    |  wait   |1|0|1|0|1|0|1|0|1|0|1|0|1|0|1|0|1|0|1|0|1|0|1|0|1|0|1|0|1|0|1
2    |download |0|1|1|0|0|1|1|0|0|1|1|0|0|1|1|0|0|1|1|0|0|1|1|0|0|1|1|0|0|1|1
3    | pause   |0|0|0|1|1|1|1|0|0|0|0|1|1|1|1|0|0|0|0|1|1|1|1|0|0|0|0|1|1|1|1
4    |finished |0|0|0|0|0|0|0|1|1|1|1|1|1|1|1|0|0|0|0|0|0|0|0|1|1|1|1|1|1|1|1
5    | error   |0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1
---------------|-------------------------------------------------------------
status of movie|1|2|2|3|1|2|2|4|1|2|2|3|1|2|2|5|1|2|2|3|1|2|2|5|1|2|2|3|1|2|2
```
 * “status of file”的第一列“10000”表示子文件所有状态都是wait，依此类推
 * 根据规则计算出 status of movie 为1(等待)，依此类推
 * 灵感来自于电工电路设计、离散数学
[slide]
# 代码实现
```java
int[] statusOfMovie = [-1,1,2,2,3,1,2,2,4,1,2,2,3,1,2,2,5,1,2,2,3,1,2,2,5,1,2,2,3,1,2,2];

int getStatusOfMovie(int[] statusOfFiles)
{		
	index = 0;
	for(int i=0; i<statusOfFiles.length; i++)
	{
		index = index | (1 << (statusOfFiles[i]-1));
	}
	return statusOfMovie[index];
}
```
- 假设三个文件的电影，文件状态:2,3,3(一个download，两个pause)
 * 状态转化成二进制表示: 2,3,3 -> 0010,0100,0100
 * 计算index = 0010|0100|0100 = 0110，即十进制的6
 * 计算最终状态: statusOfMovie[6] = 2 (download)
- 算法优点:代码量少，便于维护，效率高

[slide]
# 域名调度算法
<small>2011-01-26 siteware(使用HBase)</small>

[slide]
# 项目背景
- 需要为项目提供从IP到域名的反向解析功能
 * 先对域名(数百万)进行DNS正向查询
 * 单台服务器压力大，时间常，需要多台Local DNS查询服务器
 * 并行查询
- 需要一个好的域名下发分配算法，至少满足：
 * 尽可能实时更新域名
 * 排除无效的域名
 * 根据域名热点调整更新策略

[slide]
# 算法描述
- 为域名添加属性
 * dispatch_ts记录最近下发时间
 * ttl记录下发间隔
- 任务开始，创建下发列表，读取域名列表
- 如果 (now - dispatch_ts) > ttl 则加入下发列表
- 更新dispatch_ts成当前时间
- 每批次下发1000个域名
- ttl是不固定的，需要根据已有IP数量进行调整
 * 0个IP，则说明未查询过，设置ttl为0，表示立即查询
 * 1个IP，可能是小型网站，设置ttl为3天
 * 大于1个IP，可能是大型网站，设置超时时间为1天

[slide]
# 算法流程图
----
![](/img/domain.dispatch.jpg)

[slide]
# bengine
## 短信增值业务处理引擎
<small>2006</small>

[slide]
# 项目背景
- 四川移动项目
- 实时处理短信增值业务
- 当时最大处理性能200条/s，要求提高到500条/s
- 最终性能达到了4000条/s

[slide]
# 线程设计
- 多线程+reactor，每个线程一个reactor
- reactor: non-blocking(记得处理EMFILE)
- 线程数固定，避免频繁创建与销毁，也方便线程间调配负载
- 对实时性有要求的connection单独用一个线程；
- 数据量大的connection可以独占一个线程；
- 其他次要的辅助性connections可以共享一个线程
- 纯计算线程不用reactor(浪费)，`blocking_queue<T>`实现线程池

[slide]
# 并发设计
- 去掉不必要的锁,对象修改操作挪到同一个线程
- 使用高级并发编程构件TaskQueue
- 判断线程死活的状态变量(volatile+SIGALRM + xx_cond_timedwait())
- 丢弃信号量
 * 使用mutex:pthreads,   
   mutex采用futex实现，不必每次陷入系统调用
 * Windows的CRITICAL_SECTION 也是类似
 * semaphore一般是系统调用

[slide]
# 其它设计
- 控制热点路径的加锁粒度与加锁范围
 * 共享的数据结构里移除元素包括两步：remove+delete，  
   delete(销毁)可以放到锁外面
 * log-先判断日志级别后加锁
- 不足：使用共享内存，移植性不好

[slide]
# 后台支撑系统优化
## 监控、统计、配置下发
<small>2008 p2sp/gms/gvss</small>

[slide]
# 项目背景
- CDN之视频直播系统的支撑子系统
- 参与十七大、嫦娥奔月、温总理网友见面、国庆阅兵、春晚视频直播
- 温总理网友见面
 * 2009.2.28 流量：标准200G + p2p放大100G  
   300万人次 并发15w
 * 2010.2.27 流量：标准35G + p2p放大35G  
   并发178,000人
- 零差错控制

[slide]
# 经验总结
- 将原来脚本上传的数据改成tcp实时传输
 * 脚本实时性较差，纠错能力较差
 * 新协议对每个消息都有应答，保证消息可靠性
- 数据收集与数据入库分开
 * 降低耦合，便于扩展
 * 多进程方式，提高系统吞吐率
- 数据统计之后再入库
 * 减少数据库存储、查询等压力
 * 提高入库效率
 * 原始日志使用率较低，保存在文件系统已经满足要求

[slide]
# Thanks
